AWSTemplateFormatVersion: '2010-09-09'
Description: 'Logo Detection System Infrastructure - SQS, Lambda, and Bedrock Data Automation'

Parameters:
  EnvironmentName:
    Type: String
    Default: 'dev'
    Description: Environment name (dev, staging, prod)
  
  BedrockOutputBucket:
    Type: String
    Description: S3 bucket for Bedrock Data Automation outputs
    Default: 'logo-detection-bedrock-outputs'

Resources:
  # S3 Bucket for Bedrock Data Automation outputs
  BedrockOutputBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub '${BedrockOutputBucket}-${EnvironmentName}-${AWS::AccountId}'
      VersioningConfiguration:
        Status: Enabled
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      LifecycleConfiguration:
        Rules:
          - Id: DeleteOldResults
            Status: Enabled
            ExpirationInDays: 30

  # SQS Queue for logo detection jobs
  LogoDetectionQueue:
    Type: AWS::SQS::Queue
    Properties:
      QueueName: !Sub 'logo-detection-queue-${EnvironmentName}'
      VisibilityTimeoutSeconds: 900  # 15 minutes (Lambda timeout + buffer)
      MessageRetentionPeriod: 1209600  # 14 days
      ReceiveMessageWaitTimeSeconds: 20  # Long polling
      RedrivePolicy:
        deadLetterTargetArn: !GetAtt LogoDetectionDLQ.Arn
        maxReceiveCount: 3

  # Dead Letter Queue for failed messages
  LogoDetectionDLQ:
    Type: AWS::SQS::Queue
    Properties:
      QueueName: !Sub 'logo-detection-dlq-${EnvironmentName}'
      MessageRetentionPeriod: 1209600  # 14 days

  # IAM Role for Lambda function
  LogoDetectionLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub 'LogoDetectionLambdaRole-${EnvironmentName}'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: SQSAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - sqs:ReceiveMessage
                  - sqs:DeleteMessage
                  - sqs:GetQueueAttributes
                Resource: 
                  - !GetAtt LogoDetectionQueue.Arn
                  - !GetAtt LogoDetectionDLQ.Arn
        - PolicyName: BedrockAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - bedrock:*
                  - bedrock-data-automation:*
                Resource: '*'
        - PolicyName: S3Access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                  - s3:DeleteObject
                Resource: 
                  - !Sub '${BedrockOutputBucket}/*'
              - Effect: Allow
                Action:
                  - s3:ListBucket
                Resource: !Ref BedrockOutputBucket

  # Lambda function for processing logo detection jobs
  LogoDetectionLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub 'logo-detection-processor-${EnvironmentName}'
      Runtime: nodejs18.x
      Handler: index.handler
      Role: !GetAtt LogoDetectionLambdaRole.Arn
      Timeout: 900  # 15 minutes
      MemorySize: 1024
      Environment:
        Variables:
          AWS_REGION: !Ref AWS::Region
          BEDROCK_OUTPUT_BUCKET: !Ref BedrockOutputBucket
          ENVIRONMENT: !Ref EnvironmentName
      Code:
        ZipFile: |
          // AWS Lambda function for processing logo detection jobs from SQS
          const { BedrockDataAutomationClient, StartDataAutomationJobCommand } = require('@aws-sdk/client-bedrock-data-automation');
          const { createClient } = require('@supabase/supabase-js');

          // Initialize Bedrock Data Automation client
          const bedrockClient = new BedrockDataAutomationClient({
            region: process.env.AWS_REGION || 'us-east-1'
          });

          exports.handler = async (event) => {
            console.log('Processing SQS messages:', JSON.stringify(event, null, 2));
            
            const results = [];
            
            for (const record of event.Records) {
              try {
                const message = JSON.parse(record.body);
                console.log('Processing message:', message);
                
                const result = await processLogoDetectionJob(message);
                results.push({
                  messageId: record.messageId,
                  status: 'success',
                  result
                });
                
              } catch (error) {
                console.error('Error processing message:', error);
                results.push({
                  messageId: record.messageId,
                  status: 'error',
                  error: error.message
                });
                
                // Let SQS handle retries and DLQ
                throw error;
              }
            }
            
            return {
              statusCode: 200,
              body: JSON.stringify({
                message: 'Processed messages',
                results
              })
            };
          };

          async function processLogoDetectionJob(message) {
            const {
              jobId,
              userId,
              fileId,
              storagePath,
              mimeType,
              originalName,
              supabaseUrl,
              supabaseServiceKey
            } = message;
            
            // Initialize Supabase client
            const supabase = createClient(supabaseUrl, supabaseServiceKey);
            
            try {
              // Update job status to processing
              await updateJobStatus(supabase, jobId, 'processing');
              
              // Download image from Supabase Storage
              const { data: imageData, error: downloadError } = await supabase.storage
                .from('user-files')
                .download(storagePath);
              
              if (downloadError) {
                throw new Error(`Failed to download image: ${downloadError.message}`);
              }
              
              // Convert blob to buffer
              const imageBuffer = Buffer.from(await imageData.arrayBuffer());
              
              // Process with Bedrock Data Automation
              const detections = await processImageWithBedrock(imageBuffer, mimeType);
              
              // Save detections to database
              if (detections.length > 0) {
                await saveDetections(supabase, jobId, detections);
              }
              
              // Update job status to completed
              await updateJobStatus(supabase, jobId, 'completed');
              
              console.log(`Job ${jobId} completed successfully with ${detections.length} detections`);
              
              return {
                jobId,
                detectionsFound: detections.length,
                detections
              };
              
            } catch (error) {
              console.error(`Job ${jobId} failed:`, error);
              
              // Update job status to failed
              await updateJobStatus(supabase, jobId, 'failed', error.message);
              
              throw error;
            }
          }

          async function processImageWithBedrock(imageBuffer, mimeType) {
            try {
              // Create Bedrock Data Automation job for logo detection
              const jobInput = {
                jobName: `logo-detection-${Date.now()}`,
                jobDescription: 'Logo detection job',
                dataAutomationConfiguration: {
                  dataAutomationType: 'DOCUMENT_EXTRACTION',
                  documentExtractionConfiguration: {
                    extractionTypes: ['LOGOS'],
                    documentFormat: mimeType === 'image/png' ? 'PNG' : 'JPEG'
                  }
                },
                inputConfiguration: {
                  s3InputConfiguration: {
                    // Note: In production, you'd upload to S3 first
                    // For now, we'll use base64 inline data
                    s3Uri: 'data:' + mimeType + ';base64,' + imageBuffer.toString('base64')
                  }
                },
                outputConfiguration: {
                  s3OutputConfiguration: {
                    s3Uri: `s3://${process.env.BEDROCK_OUTPUT_BUCKET}/logo-detection-results/`
                  }
                }
              };
              
              const command = new StartDataAutomationJobCommand(jobInput);
              const response = await bedrockClient.send(command);
              
              console.log('Bedrock Data Automation job started:', response.jobArn);
              
              // Poll for job completion (simplified - in production use Step Functions)
              const jobResult = await pollForJobCompletion(response.jobArn);
              
              // Parse results and extract logo detections
              return parseLogoDetections(jobResult);
              
            } catch (error) {
              console.error('Bedrock processing error:', error);
              throw new Error(`Bedrock processing failed: ${error.message}`);
            }
          }

          async function pollForJobCompletion(jobArn, maxAttempts = 30, intervalMs = 10000) {
            // This is a simplified polling mechanism
            // In production, use Step Functions or EventBridge for better orchestration
            
            for (let attempt = 0; attempt < maxAttempts; attempt++) {
              try {
                // Check job status (you'd use DescribeDataAutomationJob here)
                // For now, simulate completion after a delay
                await new Promise(resolve => setTimeout(resolve, intervalMs));
                
                // Simulate job completion
                if (attempt >= 2) { // Simulate job taking ~30 seconds
                  return {
                    status: 'COMPLETED',
                    results: {
                      // Mock results - in real implementation, fetch from S3 output
                      logos: [
                        {
                          name: 'Nike',
                          confidence: 0.95,
                          boundingBox: { x: 0.1, y: 0.2, width: 0.3, height: 0.4 }
                        },
                        {
                          name: 'Adidas',
                          confidence: 0.87,
                          boundingBox: { x: 0.5, y: 0.6, width: 0.2, height: 0.3 }
                        }
                      ]
                    }
                  };
                }
                
              } catch (error) {
                console.error(`Job polling attempt ${attempt + 1} failed:`, error);
              }
            }
            
            throw new Error('Job polling timeout');
          }

          function parseLogoDetections(jobResult) {
            if (!jobResult.results || !jobResult.results.logos) {
              return [];
            }
            
            return jobResult.results.logos.map(logo => ({
              logoName: logo.name,
              confidence: logo.confidence,
              boundingBox: logo.boundingBox,
              metadata: {
                source: 'bedrock-data-automation',
                processedAt: new Date().toISOString()
              }
            }));
          }

          async function updateJobStatus(supabase, jobId, status, errorMessage = null) {
            const updateData = {
              status,
              updated_at: new Date().toISOString()
            };
            
            if (errorMessage) updateData.error_message = errorMessage;
            if (status === 'processing') updateData.started_at = new Date().toISOString();
            if (status === 'completed' || status === 'failed') updateData.completed_at = new Date().toISOString();
            
            const { error } = await supabase
              .from('logo_detection_jobs')
              .update(updateData)
              .eq('id', jobId);
            
            if (error) {
              console.error('Failed to update job status:', error);
              throw new Error(`Failed to update job status: ${error.message}`);
            }
          }

          async function saveDetections(supabase, jobId, detections) {
            const detectionData = detections.map(detection => ({
              job_id: jobId,
              logo_name: detection.logoName,
              confidence_score: detection.confidence,
              bounding_box: detection.boundingBox,
              detection_metadata: detection.metadata
            }));
            
            const { error } = await supabase
              .from('logo_detections')
              .insert(detectionData);
            
            if (error) {
              console.error('Failed to save detections:', error);
              throw new Error(`Failed to save detections: ${error.message}`);
            }
          }

  # Event Source Mapping to connect SQS to Lambda
  LogoDetectionEventSourceMapping:
    Type: AWS::Lambda::EventSourceMapping
    Properties:
      EventSourceArn: !GetAtt LogoDetectionQueue.Arn
      FunctionName: !Ref LogoDetectionLambda
      BatchSize: 1  # Process one message at a time for better error handling
      MaximumBatchingWindowInSeconds: 5

  # CloudWatch Log Group for Lambda
  LogoDetectionLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/lambda/logo-detection-processor-${EnvironmentName}'
      RetentionInDays: 14

  # CloudWatch Alarms for monitoring
  DLQAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: !Sub 'LogoDetection-DLQ-Messages-${EnvironmentName}'
      AlarmDescription: 'Alert when messages appear in DLQ'
      MetricName: ApproximateNumberOfVisibleMessages
      Namespace: AWS/SQS
      Statistic: Sum
      Period: 300
      EvaluationPeriods: 1
      Threshold: 1
      ComparisonOperator: GreaterThanOrEqualToThreshold
      Dimensions:
        - Name: QueueName
          Value: !GetAtt LogoDetectionDLQ.QueueName

  LambdaErrorAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: !Sub 'LogoDetection-Lambda-Errors-${EnvironmentName}'
      AlarmDescription: 'Alert on Lambda function errors'
      MetricName: Errors
      Namespace: AWS/Lambda
      Statistic: Sum
      Period: 300
      EvaluationPeriods: 2
      Threshold: 5
      ComparisonOperator: GreaterThanThreshold
      Dimensions:
        - Name: FunctionName
          Value: !Ref LogoDetectionLambda

Outputs:
  QueueUrl:
    Description: 'SQS Queue URL for logo detection jobs'
    Value: !Ref LogoDetectionQueue
    Export:
      Name: !Sub '${AWS::StackName}-QueueUrl'

  QueueArn:
    Description: 'SQS Queue ARN'
    Value: !GetAtt LogoDetectionQueue.Arn
    Export:
      Name: !Sub '${AWS::StackName}-QueueArn'

  DLQUrl:
    Description: 'Dead Letter Queue URL'
    Value: !Ref LogoDetectionDLQ
    Export:
      Name: !Sub '${AWS::StackName}-DLQUrl'

  LambdaFunctionArn:
    Description: 'Lambda function ARN'
    Value: !GetAtt LogoDetectionLambda.Arn
    Export:
      Name: !Sub '${AWS::StackName}-LambdaArn'

  BedrockOutputBucketName:
    Description: 'S3 bucket for Bedrock outputs'
    Value: !Ref BedrockOutputBucket
    Export:
      Name: !Sub '${AWS::StackName}-BedrockBucket'